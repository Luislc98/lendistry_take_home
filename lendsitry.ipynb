{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35f5ec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['202106-baywheels-tripdata.csv.zip', '202107-baywheels-tripdata.csv.zip', '202108-baywheels-tripdata.csv.zip', '202109-baywheels-tripdata.csv.zip', '202110-baywheels-tripdata.csv.zip', '202111-baywheels-tripdata.csv.zip']\n",
      "['202112-baywheels-tripdata.csv.zip', '202201-baywheels-tripdata.csv.zip', '202202-baywheels-tripdata.csv.zip', '202203-baywheels-tripdata.csv.zip', '202204-baywheels-tripdata.csv.zip', '202205-baywheels-tripdata.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "# Problem 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def fetch_trip_data():\n",
    "    url = \"https://s3.amazonaws.com/baywheels-data/\"\n",
    "    document = requests.get(url)\n",
    "    soup= BeautifulSoup(document.content,\"lxml-xml\")\n",
    "    monthlist = []\n",
    "        # extract all text embedded in the key tag as that is where the file name is located and store in a list \n",
    "    for x in soup.find_all(\"Key\"):\n",
    "        #exclude the index file from the list \n",
    "        if x.text != 'index.html':\n",
    "            monthlist.append(x.text)\n",
    "    # create a list of the last 6 file names scraped from the websites \n",
    "    last_six_months = monthlist[-6:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5730b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/luislopez/Documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory \n",
    "print(\"Current working directory: {0}\".format(cwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a19bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "for c in twelve_months:\n",
    "    csv_url = 'https://s3.amazonaws.com/baywheels-data/'+ c\n",
    "    r = requests.get(csv_url, stream =True)\n",
    "    check = zipfile.is_zipfile(io.BytesIO(r.content))\n",
    "    #check that the file is a zipfile \n",
    "    # uncompress and store zip files into local drive \n",
    "    if check:\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "08b41c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 2\n",
    "import pandas as pd\n",
    "\n",
    "unzipped_last_six_months= []\n",
    "    # remove .zip extension from file name\n",
    "for z in last_six_months:\n",
    "    \n",
    "    unzipped_last_six_months.append(z.replace(\".zip\",\"\"))\n",
    "# create pandas dataframe with combined data from the 6 csv files \n",
    "combined_df = (pd.read_csv(f) for f in unzipped_last_six_months)\n",
    "combined_df = pd.concat(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7b4a2db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1116107 entries, 0 to 232952\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   ride_id             1116107 non-null  object \n",
      " 1   rideable_type       1116107 non-null  object \n",
      " 2   started_at          1116107 non-null  object \n",
      " 3   ended_at            1116107 non-null  object \n",
      " 4   start_station_name  937971 non-null   object \n",
      " 5   start_station_id    937971 non-null   object \n",
      " 6   end_station_name    913544 non-null   object \n",
      " 7   end_station_id      913544 non-null   object \n",
      " 8   start_lat           1116107 non-null  float64\n",
      " 9   start_lng           1116107 non-null  float64\n",
      " 10  end_lat             1114821 non-null  float64\n",
      " 11  end_lng             1114821 non-null  float64\n",
      " 12  member_casual       1116107 non-null  object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 119.2+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "278dce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the started at and ended at fields to datetime types \n",
    "combined_df.started_at = pd.to_datetime(combined_df.started_at)\n",
    "combined_df.ended_at = pd.to_datetime(combined_df.ended_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "100fbd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id               1116107\n",
       "rideable_type               3\n",
       "started_at            1050175\n",
       "ended_at              1049374\n",
       "start_station_name        505\n",
       "start_station_id          499\n",
       "end_station_name          506\n",
       "end_station_id            500\n",
       "start_lat              209987\n",
       "start_lng              232130\n",
       "end_lat                 24639\n",
       "end_lng                 25124\n",
       "member_casual               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.nunique()\n",
    "#check for any red flags pertaining to unique counts of fields\n",
    "# discrepancy between the start/end station and the start/end station id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b5d4b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id                    0\n",
       "rideable_type              0\n",
       "started_at                 0\n",
       "ended_at                   0\n",
       "start_station_name    178136\n",
       "start_station_id      178136\n",
       "end_station_name      202563\n",
       "end_station_id        202563\n",
       "start_lat                  0\n",
       "start_lng                  0\n",
       "end_lat                 1286\n",
       "end_lng                 1286\n",
       "member_casual              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null value count of fields \n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "06bd5b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id               0\n",
       "rideable_type         0\n",
       "started_at            0\n",
       "ended_at              0\n",
       "start_station_name    0\n",
       "start_station_id      0\n",
       "end_station_name      0\n",
       "end_station_id        0\n",
       "start_lat             0\n",
       "start_lng             0\n",
       "end_lat               0\n",
       "end_lng               0\n",
       "member_casual         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove entries that have null values for start station or end station\n",
    "combined_df = combined_df[combined_df['start_station_id'].notnull()]\n",
    "combined_df = combined_df[combined_df['end_station_id'].notnull()]\n",
    "#check that rows were removed \n",
    "combined_df.isnull().sum()\n",
    "# appears that rows with null values in ending latitude and longitude also all had null values for either start station\n",
    "# or end station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "afb5cdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates \n",
    "combined_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7a972092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a trip_duration field that stores the calculated duration of trip in order to find any inconsistencies\n",
    "combined_df[\"trip_duration\"] = combined_df[\"ended_at\"] - combined_df[\"started_at\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c997f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out entries that have a trip duration of under a minute as they are likely user error and their inclusion could\n",
    "# affect calculations such as avg trip duration \n",
    "combined_df = combined_df[combined_df[\"trip_duration\"]> pd.Timedelta(1,'m')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "76dae0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select start station name for ids with multiple names depending on what entry is the most recent \n",
    "# this will resolve the start/end station name and start/end station id unique count discrepancy\n",
    "consolidated_start_station= combined_df.sort_values('started_at').groupby('start_station_id').tail(1)[['start_station_id','start_station_name']]\n",
    "# do same for end stations\n",
    "consolidated_end_station= combined_df.sort_values('ended_at').groupby('end_station_id').tail(1)[['end_station_id','end_station_name']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7d92bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that were used to assist in data cleaning and were not in original dataset\n",
    "#also remove end station name and start station name as they will not be in main sql table \n",
    "cleaned_combined_df = combined_df.drop(['start_station_name', 'end_station_name',\"trip_duration\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e9c7af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to csv files that will be used to be pushed to their respective SQL tables\n",
    "consolidated_end_station.to_csv('combined_end_station.csv', index=False)\n",
    "consolidated_start_station.to_csv('combined_start_station.csv', index=False)\n",
    "cleaned_combined_df.to_csv('combined_trip_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c88b280e",
   "metadata": {},
   "source": [
    "Problem 3.\n",
    "CREATE TABLE start_station \n",
    "( \n",
    "  start_station_id TEXT PRIMARY KEY,\n",
    "  start_station_name TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE end_station \n",
    "( \n",
    "  end_station_id TEXT PRIMARY KEY,\n",
    "  end_station_name TEXT\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE tripdata (\n",
    "  ride_id char(16) PRIMARY KEY,\n",
    "  rideable_type TEXT,\n",
    "  started_at TIMESTAMP ,\n",
    "  ended_at TIMESTAMP ,\n",
    "  start_station_id TEXT REFERENCES start_station (start_station_id),\n",
    "  end_station_id TEXT REFERENCES end_station (end_station_id),\n",
    "  start_lat NUMERIC ,\n",
    "  start_lng NUMERIC,\n",
    "  end_lat NUMERIC,\n",
    "  end_lng NUMERIC,\n",
    "  member_casual TEXT,\n",
    "  ...\n",
    ");\n",
    "CREATE INDEX index_date_range ON trip_data (started_at,ended_at);\n",
    "CREATE INDEX index_start_station ON trip_data(start_station_id);\n",
    "CREATE INDEX index_end_station ON trip_data(end_station_id);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a09104b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "   database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "def load_start_station():\n",
    "    try:\n",
    "        \n",
    "        with open('combined_start_station.csv', 'r') as f:\n",
    "            next(f) # Skip the header row.\n",
    "            cur.copy_from(f, 'start_station', sep=',')\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to start_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "        \n",
    "def load_end_station():\n",
    "   \n",
    "    try:\n",
    "        with open('combined_end_station.csv', 'r') as f:     \n",
    "            next(f) # Skip the header row.\n",
    "            cur.copy_from(f, 'end_station', sep=',')\n",
    "            conn.commit()  \n",
    "            conn.close()\n",
    "            \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to end_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "def trip_data():\n",
    "    \n",
    "    try:\n",
    "        with open('combined_trip_data.csv', 'r') as f:\n",
    "        \n",
    "            next(f) # Skip the header row.\n",
    "            cur.copy_from(f, 'trip_data', sep=',')\n",
    "            conn.commit()  \n",
    "            conn.close()\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to trip_data table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c2a54fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateTable",
     "evalue": "relation \"start_station\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1j/vm8k959s0p74yc6kymmj3d9m0000gn/T/ipykernel_40750/3684461414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mCREATE\u001b[0m \u001b[0mINDEX\u001b[0m \u001b[0mindex_end_station\u001b[0m \u001b[0mON\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_station_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m '''\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Table created successfully........\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateTable\u001b[0m: relation \"start_station\" already exists\n"
     ]
    }
   ],
   "source": [
    "# code used to create tables \n",
    "import psycopg2\n",
    "#Establishing the connection\n",
    "conn = psycopg2.connect(\n",
    "   database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432'\n",
    ")\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Doping EMPLOYEE table if already exists.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n",
    "\n",
    "#Creating table as per requirement\n",
    "sql ='''\n",
    "CREATE TABLE start_station (\n",
    "    start_station_id text PRIMARY KEY,\n",
    "    start_station_name TEXT\n",
    "\n",
    ");\n",
    "CREATE TABLE end_station (\n",
    "    end_station_id text PRIMARY KEY,\n",
    "    end_station_name TEXT\n",
    "\n",
    ");\n",
    "CREATE TABLE trip_data (\n",
    "  ride_id char(16) PRIMARY KEY,\n",
    "  rideable_type TEXT,\n",
    "  started_at TIMESTAMP ,\n",
    "  ended_at TIMESTAMP ,\n",
    "  start_station_id TEXT REFERENCES start_station (start_station_id),\n",
    "  end_station_id TEXT REFERENCES end_station (end_station_id),\n",
    "  start_lat NUMERIC ,\n",
    "  start_lng NUMERIC,\n",
    "  end_lat NUMERIC,\n",
    "  end_lng NUMERIC,\n",
    "  member_casual TEXT\n",
    "  \n",
    ");\n",
    "CREATE INDEX index_date_range ON trip_data(started_at,ended_at);\n",
    "CREATE INDEX index_start_station ON trip_data(start_station_id);\n",
    "CREATE INDEX index_end_station ON trip_data(end_station_id);\n",
    "'''\n",
    "cursor.execute(sql)\n",
    "print(\"Table created successfully........\")\n",
    "conn.commit()\n",
    "#Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1eb4dc",
   "metadata": {},
   "source": [
    "Design and implement an ETL process that watches the source website for new data and\n",
    "automatically downloads, cleans and loads the new data into the Postgres database.\n",
    "Show how you handle errors (e.g., connection drops while downloading the data,\n",
    "download is corrupted) and any re-try logic. (A, C)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3132632",
   "metadata": {},
   "source": [
    "Probelm 5. Implementation plan:\n",
    "    Use a python package such as schedule to have the following etl process run on a scheduled basis(given that the website seems to upload new files on random days of each month, running it every 5 days might be optimal\n",
    "   example code to schedule etl process : schedule.every(5).days.do(etl_process)\n",
    "    Handling errors: any errors that arise from with the downloading process can be handled with a Try Except statement, a rough implementation is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    # try to request to url with a timeout limit added as a parameter \n",
    "    etl_process = requests.get(csv_url,timeout=.05)\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"Connection Error. \")\n",
    "    print(str(e))            \n",
    "    \n",
    "except requests.Timeout as e:\n",
    "    print(\"Timeout Error\")\n",
    "    print(str(e))\n",
    "    \n",
    "except requests.RequestException as e:\n",
    "    print(\"General Error\")\n",
    "    print(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "da3db48e",
   "metadata": {},
   "source": [
    "Retry logic: using the retry decorator from the retry package, create  a function that would try to fetch data from the website x amount of times with a delay of n seconds in between each attempt.\n",
    "The function's parameters would be  @retry((ConnectionError,Timeout,RequestException), delay=n, tries=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce144a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8079794c",
   "metadata": {},
   "source": [
    " the main portion of the etl process is below(NOTE: the exception handling and retry logic has not been implemented.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9878414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 826339 entries, 0 to 183675\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   ride_id           826339 non-null  object        \n",
      " 1   rideable_type     826339 non-null  object        \n",
      " 2   started_at        826339 non-null  datetime64[ns]\n",
      " 3   ended_at          826339 non-null  datetime64[ns]\n",
      " 4   start_station_id  826339 non-null  object        \n",
      " 5   end_station_id    826339 non-null  object        \n",
      " 6   start_lat         826339 non-null  float64       \n",
      " 7   start_lng         826339 non-null  float64       \n",
      " 8   end_lat           826339 non-null  float64       \n",
      " 9   end_lng           826339 non-null  float64       \n",
      " 10  member_casual     826339 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(5)\n",
      "memory usage: 75.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# the list of existing files downloaded from the website\n",
    "global_list= []\n",
    "def get_new_trip_data_file_names(existing):\n",
    "    newfiles= []\n",
    "    url = \"https://s3.amazonaws.com/baywheels-data/\"\n",
    "    document = requests.get(url)\n",
    "    soup= BeautifulSoup(document.content,\"lxml-xml\")\n",
    "    monthlist = []\n",
    "        # extract all text embedded in the key tag as that is where the file name is located and store in a list \n",
    "    for x in soup.find_all(\"Key\"):\n",
    "        #exclude the index file from the list and check if there is any new file in the website\n",
    "        if x.text != 'index.html' and x.text not in existing:\n",
    "            # make the global_list variable global so it can add any new files that will be processed into the list of\n",
    "            # existing files \n",
    "            global global_list\n",
    "            global_list.append(x.text)\n",
    "            newfiles.append(x.text)\n",
    "            \n",
    "        # if there are newfile, proceed to downloading and processing them \n",
    "    if len(newfiles)>0:\n",
    "        print(\"new data found\")\n",
    "        #process_zip_files(newfiles)\n",
    "    else:\n",
    "        print(\"no new data file found\")\n",
    "        \n",
    "    # download and unzip the new files that were found into local drive \n",
    "def process_zip_files(zip_file_list):\n",
    "    unzipped_file_list= []\n",
    "    #create list of files that are successfully unzipped and extracted that\n",
    "    for c in zip_file_list:\n",
    "        url = 'https://s3.amazonaws.com/baywheels-data/'+ c\n",
    "        r = requests.get(url, stream =True)\n",
    "        check = zipfile.is_zipfile(io.BytesIO(r.content))\n",
    "        #check that the file is a zipfile \n",
    "        if check:\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            # unzip the zip file \n",
    "            z.extractall()\n",
    "            unzipped_file_list.append(c)\n",
    "    \n",
    "    #remove zip extension from file name\n",
    "    for f in unzipped_file_list:\n",
    "            unzipped_file_list.append(f.replace(\".zip\",\"\"))\n",
    "    clean_new_data(unzipped_file_list)\n",
    "    \n",
    "def get_end_station():\n",
    "   \n",
    "    try:\n",
    "        \n",
    "        conn = psycopg2.connect(database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432')\n",
    "        cur = conn.cursor()\n",
    "        postgreSQL_select_Query = \"select * from end_station\"\n",
    "        cur.execute(postgreSQL_select_Query) \n",
    "        end_station_query = cur.fetchall()      \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to end_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return end_station_query\n",
    "\n",
    "def get_start_station():\n",
    "   \n",
    "    try:\n",
    "        \n",
    "        conn = psycopg2.connect(database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432')\n",
    "        cur = conn.cursor()\n",
    "        postgreSQL_select_Query = \"select * from start_station\"\n",
    "        cur.execute(postgreSQL_select_Query) \n",
    "        start_station_query = cur.fetchall()      \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to start_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            return start_station_query \n",
    "    \n",
    "    \n",
    "def clean_new_data(unzipped_file_list):    \n",
    "    new_combined_df = (pd.read_csv(f) for f in unzipped_file_list)\n",
    "    new_combined_df = pd.concat(new_combined_df)\n",
    "    \n",
    "    # convert the started at and ended at fields to datetime types \n",
    "    new_combined_df.started_at = pd.to_datetime(new_combined_df.started_at)\n",
    "    new_combined_df.ended_at = pd.to_datetime(new_combined_df.ended_at)\n",
    "    \n",
    "    #remove entries that have null values for start station or end station\n",
    "    new_combined_df = new_combined_df[new_combined_df['start_station_id'].notnull()]\n",
    "    new_combined_df = new_combined_df[new_combined_df['end_station_id'].notnull()]\n",
    "    \n",
    "    # create a trip_duration field that stores the calculated duration of trip in order to find any inconsistencies\n",
    "    new_combined_df[\"trip_duration\"] = new_combined_df[\"ended_at\"] - new_combined_df[\"started_at\"]\n",
    "    \n",
    "    #filter out entries that have a trip duration of under a minute as they are likely user error and their inclusion could\n",
    "    # effect calculations such as avg trip duration \n",
    "    new_combined_df = new_combined_df[new_combined_df[\"trip_duration\"]> pd.Timedelta(1,'m')]\n",
    "    cleaned_new_combined_df = new_combined_df.drop(['start_station_name', 'end_station_name',\"trip_duration\"], axis = 1)\n",
    "    \n",
    "    #select start station name for ids with multiple names depending on what entry is the most recent \n",
    "    new_start_station= new_combined_df.sort_values('started_at').groupby('start_station_id').tail(1)[['start_station_id','start_station_name']]\n",
    "    # do same for end stations\n",
    "    new_end_station= new_combined_df.sort_values('ended_at').groupby('end_station_id').tail(1)[['end_station_id','end_station_name']]\n",
    "    \n",
    "    \n",
    "    # read existing list of unique end station names/ids and right join with list obtained from new data \n",
    "      \n",
    "    check_df_end = pd.DataFrame(get_end_station())\n",
    "    df_end_station_join = check_df_end.merge(new_end_station.drop_duplicates(), on='end_station_id', \n",
    "                   how='right', indicator=True)\n",
    "\n",
    "    df_end_station_join= df_end_station_join[df_end_station_join['_merge'] == 'right_only']\n",
    "    # check if there any right join values, as those would be values that would need to be pushed to end/start station table\n",
    "    if not df_end_station_join.empty:\n",
    "        df_end_station_join = df_end_station_join.drop(['_merge'], axis = 1)\n",
    "        df_end_station_join = df_end_station_join.filter(['end_station_id', 'end_station_name_y'])\n",
    "        df_end_station_join.columns = ['end_station_id','end_station_name']\n",
    "        # add right join data to new csv file to be added to end station table \n",
    "        df_end_station_join.to_csv('new_end_station_data.csv', index=False)\n",
    "        load_new_end_station_data()\n",
    "        \n",
    "    check_df_start = pd.DataFrame(get_start_station())\n",
    "    df_start_station_join = check_df_start.merge(new_start_station.drop_duplicates(), on='start_station_id', \n",
    "                   how='right', indicator=True)\n",
    "    \n",
    "    df_start_station_join= df_start_station_join[df_start_station_join['_merge'] == 'right_only']\n",
    "    #check if there any right join values\n",
    "    if not df_start_station_join.empty:\n",
    "        df_start_station_join = df_start_station_join.drop(['_merge'], axis = 1)\n",
    "        df_start_station_join = df_start_station_join.filter(['start_station_id', 'start_station_name_y'])\n",
    "        df_start_station_join.columns = ['start_station_id','start_station_name']\n",
    "        # add right join data to new csv file to be added to start station table \n",
    "        df_start_station_join.to_csv('new_start_station_data.csv', index=False)\n",
    "        load_new_start_station_data()\n",
    "    # check that the main dataframe is not empty \n",
    "    if not cleaned_new_combined_df.empty:\n",
    "        # push cleaned data to local hard drive to later be loaded to the main trip_data table \n",
    "        cleaned_new_combined_df.to_csv('new_main_data.csv', index=False)\n",
    "        load_new_trip_data()\n",
    "    # push new data to trip data table\n",
    "def load_new_trip_data():\n",
    "    try:\n",
    "        conn = psycopg2.connect(database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432')\n",
    "        cur = conn.cursor()\n",
    "        with open('new_main_data.csv', 'r') as f:\n",
    "            next(f) \n",
    "            cur.copy_from(f, 'trip_data', sep=',')\n",
    "            conn.commit()  \n",
    "            \n",
    "            \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to trip_data table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"new trip data pushed successfully!\")\n",
    "            \n",
    "    # push new data to end station table\n",
    "def load_new_end_station_data():\n",
    "    try:\n",
    "        conn = psycopg2.connect(database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432')\n",
    "        cur = conn.cursor()\n",
    "        with open('new_end_station_data.csv', 'r') as f:\n",
    "            next(f) \n",
    "            cur.copy_from(f, 'end_station', sep=',')\n",
    "            conn.commit()  \n",
    "            print(\"new end station  data pushed successfully!\")\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to end_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"new end_station data pushed successfully!\")\n",
    "        # push new data to start station table\n",
    "def load_new_start_station_data():\n",
    "    try:\n",
    "        conn = psycopg2.connect(database=\"postgres\", user='newuser', password='password', host='localhost', port= '5432')\n",
    "        cur = conn.cursor()\n",
    "        with open('new_start_station_data.csv', 'r') as f:\n",
    "            next(f) \n",
    "            cur.copy_from(f, 'start_station', sep=',')\n",
    "            conn.commit()  \n",
    "            print(\"new start station  data pushed successfully!\") \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error pushing data to start_station table\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"new start_station data pushed successfully!\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c8d9fd3",
   "metadata": {},
   "source": [
    "10. \n",
    "Query 1: queries the average trip duration \n",
    "Select  avg(trip_data.ended_at - trip_data.started_at ) as avg_route_duration\n",
    "from trip_data \n",
    "\n",
    "Query 2: queries the 5 most popular start stations \n",
    "Select start_station.start_station_name, Count(start_station.start_station_name) as start_station_count from trip_data\n",
    "INNER JOIN start_station ON trip_data.start_station_id=start_station.start_station_id\n",
    "group by start_station.start_station_name\n",
    "Order By start_station_count Desc limit 5;\n",
    "\n",
    "Query 3: queries the 5 most popular end stations \n",
    "Select end_station.end_station_name, count(end_station.end_station_name) as end_station_count from trip_data\n",
    "INNER JOIN end_station ON trip_data.end_station_id=end_station.end_station_id\n",
    "group by end_station.end_station_name Order By end_station_count Desc limit 5;\n",
    "\n",
    "Query 4: queries the top 5 most popular routes\n",
    "Select  start_station.start_station_name, \n",
    "end_station.end_station_name, Count(trip_data.start_station_id) as round_trip_count from trip_data\n",
    "INNER JOIN start_station ON trip_data.start_station_id=start_station.start_station_id\n",
    "INNER JOIN end_station ON trip_data.end_station_id=end_station.end_station_id\n",
    "group by start_station.start_station_name, end_station.end_station_name\n",
    "Order By round_trip_count Desc limit 5 ;\n",
    "\n",
    "Query 5: queries the count of rides by weekday and the average duration of a ride per weekday\n",
    "\n",
    "Select  to_char(started_at, 'dy') as weekday,avg(trip_data.ended_at - trip_data.started_at ) as avg_route_duration_weekday,\n",
    "count(to_char(started_at, 'dy')) as weekday_count\n",
    "from trip_data group by weekday order by weekday_count desc;"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae3cdf9",
   "metadata": {},
   "source": [
    "12. \n",
    "\n",
    "The derived tables will be maintained through the use of the Alteryx tool, specifically a macro , as it is a workflow that can be scheduled to run at various points in time. This workflow will be scheduled to run every 5 days.It would have logic that checks a folder for a csv file(this workflow would be dependent on the etl process that would push a cleaned data csv file to this folder whenever it detects new data on the website) and continues if the folder  is not empty(at the end of the workflow it would move the file to a different folder to ensure only new data is pushed). As new data is provided on a roughly monthly basis, new data would not be needed to be pushed to the derived tables immediately. Data would be pushed to the derived tables incrementally.  \n",
    "\n",
    "This workflow would take the data in the file and create the fields based on existing fields for the two derived table. It would then append the resulting data to the two derived tables. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The trip_distance_data table will be used in order to create visualizations regarding the trips' locations, whether it be a map doing geogrpahical analysis of the trip data, or analysis of the distance through charts/graphs.\n",
    "\n",
    "The trip_time_data table will assist in the analysis and creation of dashbboards regarding trip acitivy, such as graphs displaying daily/monthly activity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4fe730a",
   "metadata": {},
   "source": [
    "Derived Tables:\n",
    "\n",
    "\n",
    "CREATE TABLE trip_distance_data (\n",
    "  ride_id char(16) PRIMARY KEY,\n",
    "  start_station_id TEXT REFERENCES start_station (start_station_id),\n",
    "  end_station_id TEXT REFERENCES end_station (end_station_id),\n",
    "  start_lat NUMERIC ,\n",
    "  start_lng NUMERIC,\n",
    "  end_lat NUMERIC,\n",
    "  end_lng NUMERIC,\n",
    "  distance_km NUMERIC,\n",
    "  distance_miles NUMERIC,\n",
    " \n",
    ");\n",
    "CREATE TABLE trip_time_data (\n",
    "  ride_id char(16) PRIMARY KEY,\n",
    "  start_station_id TEXT REFERENCES start_station (start_station_id),\n",
    "  end_station_id TEXT REFERENCES end_station (end_station_id),\n",
    "  day TEXT,\n",
    "  month TEXT,\n",
    "  year TEXT,\n",
    "  duration_minutes INTEGER\n",
    " \n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bd2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
